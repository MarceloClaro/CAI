선형회귀분석을 사용하기 위한 조건

1. 독립변수값에 해당하는 종족변수 값들은 정규분포를 이뤄야하고 모든 정규분포의 분산은 동일해야한다
	- 타이타닉 예제의 변수도 정규분포가 아니라고 생각되는데 확인해보자
	
	
2. 종속변수값들은 통계적으로 서로 독립적이어야 한다
	- 종속변수들이 서로 독립적인 것을 통계적으로 증명하려면?
		: 회귀분석에서 Durbin-watson값을 제시하여 Durbin-watson값이 2에서 크게 멀지 않다는 것을 제시
		
		- Durbin-Watson의 경우 머신러닝을 돌리면 값이 나옴


3. 다중회귀분석의 경우 독립변수끼리 다중공선성이 존재하지 않아야 한다
	- 다중공선성이란? : 나이와 학년이 우울감에 미치는 영향을 살펴보고자 하면 나이와 학년은 거의 같은 변수이므로,
				독립변인끼리 거의 같은 개념이라고 볼수있습니다. 
	- 실제로 통계분석을 수행할때는 VIF(분산팽창계수)가 10이상일때 다중공선성이 존재한다고 판단
	
		- 그래서 아마 사람들이 퍼스널컬러에 관하여 가지고 있는 인식과, 원하는 인식의 답이 거의 흡사하므로
		  아마도 다중공선성 수치가 높지않을까?싶음. 둘중에 하나만 골라서 사용해야 할듯





** 결과표 보는 방법


- R-squared(R제곱) 값이 0.589이다. R-squared는 앞서 회귀분석을 실시한 "height  = B0 + B1 * weight" 모델이 body 데이터프레임을 얼마나 설명해주는지 모델식의 적합성을 말해준다. 결과는 선형 회귀분석 모델이 height 변동성의 58.9%를 설명한다는 의미이다. R-squared는 0 ~ 1의 값을 가지고 0 이면 모델의 설명력이 전혀 없는 상태, 1이면 모델이 완벽하게 데이터를 설명해주는 상태이다. 사회과학에서는 보통 0.4 이상 이면 괜찮은 모델이라고 보면 된다.

- coef(coefficient, 계수)를 살펴보면, intercept는 107.8624, weight는 0.8904이다. 이 계수값을 선형 회귀분석 모델에 대입하면 "height  = 107.8624 + 0.8904 * weight"의 식이 산출된 것을 확인할 수 있다.

- P>|t|(유의확률)은 독립변수의 유의확률이다. 보통 독립변수가 95%의 신뢰도를 가져야 유의미하다고 판단한다. 이 경우 독립변수의 유의확률은 0.05보다 작은 값이 산출된다. 즉, 독립변수의 유의확률이 0.05보다 작으면, 독립변수가 종속변수에 영향을 미치는 것이 유의미하다고 본다. 위 경우 weight의 유의확률은 0이다. 따라서 weight는 height에 유의미하게 영향을 미친다고 할 수 있다.

- Durbin-Watson(더빈왓슨, DW검정)의 값은 2.201이다. DW검정은 잔차의 독립성을 확인할 수 있는 수치이다. 0이면 잔차들이 양의 자기상관을 갖고, 2이면 자기상관이 없는 독립성을 갖고, 4이면 잔차들이 음의 자기상관을 갖는다고 해석한다. 보통 1.5 ~ 2.5사이이면 독립으로 판단하고 회귀모형이 적합하다는 것을 의미한다. DW검정값이 0 또는 4에 가깝다는 것은 잔차들이 자기상관을 가지고 있다는 의미이고, 이는 t값, F값, R제곱을 실제보다 증가시켜 실제로 유의미하지 않은 결과를 유의미한 결과로 왜곡하게 된다.

- No.observations는 24이다. 즉 24개의 데이터 쌍을 가지고 회귀분석을 실시하였다는 것을 알 수 있다.

- Df Model은 1이다. Df Model은 회귀분석의 "예측변수의 숫자(k)"를 의미한다. 회귀분석의 전체 파라미터는 1개의 종속변수를 포함하므로 Df Model은 다른 식으로는 "회귀분석 전체 파라미터 숫자 - 1" 이다. 예시 회귀모형에서 예측변수는 "weight" 1개이므로 Df Model은 1이 된다.

- Df Residuals는 22이다. Df Residuals는 "No.observations - (Df Model + 1)"로 산출한다. 즉 전체 관찰데이터의 수에서 회귀모형의 전체 파라미터의 수를 뺀 값이다. 우리 모형에서는 24 - 2 = 22로 산출되었다.




(참조)

https://m.blog.naver.com/PostView.nhn?blogId=moses3650&logNo=221181224517&proxyReferer=https%3A%2F%2Fwww.google.com%2F

https://kiyoja07.blogspot.com/2019/03/python-linear-regression.html